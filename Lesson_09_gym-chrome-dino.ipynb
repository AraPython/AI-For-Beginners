{"cells":[{"cell_type":"markdown","source":["The input state of the Lunar Lander consists of following components:\n","\n","  1. Horizontal Position\n","  2. Vertical Position\n","  3. Horizontal Velocity\n","  4. Vertical Velocity\n","  5. Angle\n","  6. Angular Velocity\n","  7. Left Leg Contact\n","  8. Right Leg Contact\n","\n","The actions of the agents are:\n","  1. Do Nothing\n","  2. Fire Main Engine\n","  3. Fire Left Engine\n","  4. Fire Right Engine\n","\n","\n","<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/static/lunar_lander.png\">"],"metadata":{"id":"j6QqH4KEFQz4"}},{"cell_type":"code","source":["!pip install Box2D\n","!pip install box2d-py\n","!pip install gym[all]\n","!pip install gym[Box_2D]"],"metadata":{"id":"xiWmqJiNCrQ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7krM1dWoP_Cf"},"outputs":[],"source":["import gym\n","from gym.wrappers import Monitor\n","env = env=gym.make(\"LunarLander-v2\")\n","state = env.reset() # reset environment to a new, random state\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))\n","action = env.action_space.sample() # Explore action space\n","print(action)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uM-Cx5pOQsVT"},"outputs":[],"source":["import numpy as np\n","q_table = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1VytvNWQvDA"},"outputs":[],"source":["%%time\n","\"\"\"Training the agent\"\"\"\n","import random\n","from IPython.display import clear_output\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","          try:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","          except:\n","            q_table[state]=[0,0,0,0]\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action) #explain more\n","        old_value = q_table[state][action]\n","        try:\n","          next_max = np.max(q_table[next_state])\n","        except:\n","          q_table[state]=[0,0,0,0]\n","          next_max = np.max(q_table[next_state])\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        if state in q_table.keys():\n","          q_table[state][action] = new_value\n","        else:\n","          q_table[state]=[0,0,0,0]\n","          q_table[state][action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","        state = next_state\n","        epochs += 1\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bB-Z4rreQ3Uq"},"outputs":[],"source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","frames1 = [] # for animation\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    done = False\n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","        if reward == -10:\n","            penalties += 1\n","        # Put each rendered frame into dict for animation\n","        frames1.append({\n","            'frame': env.render(mode='ansi'),\n","            'state': state,\n","            'action': action,\n","            'reward': reward\n","            }\n","        )\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Lesson_09_gym-chrome-dino.ipynb","provenance":[{"file_id":"1OCYQjiZE4RSDwpDjYvdDu10mUxJtQyN1","timestamp":1656490375379}],"private_outputs":true,"authorship_tag":"ABX9TyMLwWt0k5EEm+10AIa0PIzw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}